---
title: "test-estimate-accuracy"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r echo=FALSE}
library(infotheo)
library(tidyverse)
library(ggplot2)
library(devtools)
library(standardPrintOutput)
#library(tidyinfostats)
devtools::load_all("..")
set.seed(101)
theme_set(standardPrintOutput::defaultFigureLayout())
```



## Define test distributions

### gaussians
gaussian test params
y0 = [ .4 .5 .8 ];          % the center of the gaussian
sigma_y = [ .2 .3 .25 ];    % the gaussian decay constant
p{2} = [ .2 1 0.5 ];        % the (normalized) amplitude p(x)

```{r}
gaussians = ConditionalDistribution$new()
gaussians$withDistribution(0.2,NormalDistribution$new(0.4,0.2))
gaussians$withDistribution(1,NormalDistribution$new(0.5,0.3))
gaussians$withDistribution(0.5,NormalDistribution$new(0.8,0.25))
```

### Uniform distributions
square wave test params

a = [ 0 .1 .2 ];            % the left side of each square wave
b_a = [ 1 1.1 1.1 ];        % the length in y of each square wave
p{1} = [ .2 1 0.5 ];        % the (normalized) amplitude p(x)

```{r}
squareWaves = ConditionalDistribution$new()
squareWaves$withDistribution(0.2,UniformDistribution$new(min=0,max=1))
squareWaves$withDistribution(1,UniformDistribution$new(min=0.1,max=1.2))
squareWaves$withDistribution(0.5,UniformDistribution$new(min=0.2,max=1.3))
```

### Log normals

```{r}
lognorm = ConditionalDistribution$new()
lognorm$withDistribution(0.5,LogNormalDistribution$new(mode=0.5,sd=0.25))
lognorm$withDistribution(1,LogNormalDistribution$new(mode=1.5,sd=1))
lognorm$withDistribution(0.3,LogNormalDistribution$new(mode=2.5,sd=0.5))
```

### plot our distributions

```{r}
gaussians$plot(-1,2)
standardPrintOutput::saveThirdPageFigure(filename="~/Dropbox/featureSelection/mutinfo/gaussiansDist")
```

```{r}
squareWaves$plot(-0.5,1.5)
standardPrintOutput::saveThirdPageFigure(filename="~/Dropbox/featureSelection/mutinfo/squareDist")
```

```{r}
lognorm$plot(0,5)
standardPrintOutput::saveThirdPageFigure(filename="~/Dropbox/featureSelection/mutinfo/lognormDist")
```

## Experiment 0

We are using KWindow instead of KNN in most of this for performance

We need to know how good (or otherwise) this compares to estimate of KNN 

```{r results = "hide", cache=TRUE}
#experiment0 = function() {
set.seed(101)
devtools::load_all("..")
exp0Data = NULL
for (i in c(1:100)) {
  cd = ConditionalDistribution$new()
  cd$withRandomDistributions()
  thMi = cd$theoreticalMI()
  #thMu = cd$theoreticalMean()
  #thSd = sqrt(cd$theoreticalVariance())
  
  for (j in c(1:30)) {
    sampleSize = sample.int(480,1)+20
    df = cd$sample(sampleSize)
    mi.knn = calculateDiscreteContinuousMI(df, y, x, method = "KNN") %>% pull(I)
    mi.kwindow = calculateDiscreteContinuousMI(df, y, x, method = "KWindow") %>% pull(I)
    exp0Data = exp0Data %>% rbind(tibble(
      sampleSize = sampleSize,
      mi.knn = mi.knn,
      mi.kwindow = mi.kwindow,
      mi.theoretical = thMi,
      i = i,
      j=j
    ))
  }
  
}


```

Predicted KNN versus Predicted KWindow plot shows a reasonable agreement between KNN and KWindow

```{r}
exp0Data.summ = exp0Data %>% mutate(
  mi.delta = mi.kwindow-mi.knn,
  mi.relative_error = mi.delta
)

ggplot(exp0Data.summ, aes(x=mi.kwindow, y=mi.knn))+
  geom_point(alpha=0.1,size=1,stroke=0)+geom_abline(slope=1,colour="grey")+
  coord_cartesian(x=c(0,1),y=c(0,1))+ylab("Est. MI (KN method)")+xlab("Est. MI (KWindow method)")
standardPrintOutput::saveSixthPageFigure(filename="~/Dropbox/featureSelection/mutinfo/KnnVsKwindow")

```

Predicted KNN versus Predicted KWindow delta converges with increasing sample size.

```{r}
ggplot(exp0Data.summ, aes(x=sampleSize, y=mi.relative_error))+geom_point(alpha=0.5,stroke=0,size=1) + 
  geom_smooth(method='lm', formula= y~x)+ylab("\u0394 est MI (KWindow-KNN)")+xlab("Sample size")
standardPrintOutput::saveSixthPageFigure(filename="~/Dropbox/featureSelection/mutinfo/KnnVsKwindowErrorVsSampleSize")
```

Absolute estimate difference increases with theoretical value of MI - i.e. MI error size is proportional to MI.
N>B> that we don't know which (KNN or KWindow) is better against theoretical MI at this stage.

```{r}

exp0Data.summ2 = exp0Data.summ %>% 
  group_by(i,mi.theoretical) %>% 
  summarise(
    mi.mean_rel_error=mean(abs(mi.relative_error),na.rm = TRUE), 
    mi.sd_rel_error=sd(abs(mi.relative_error),na.rm = TRUE)
  ) %>% mutate(
    x=mi.theoretical,
    y=mi.mean_rel_error,
    ymin=mi.mean_rel_error-1.96*mi.sd_rel_error, 
    ymax=mi.mean_rel_error+1.96*mi.sd_rel_error
  )

ggplot(exp0Data.summ2, aes(x=x,y=y))+geom_point()+ylab("|\u0394 MI|")+xlab("MI (Theoretical)")+geom_errorbar(
  aes(ymin=ifelse(ymin<0,0,ymin), ymax=ymax)
) + geom_smooth(method='lm', formula= y~x)
standardPrintOutput::saveSixthPageFigure(filename="~/Dropbox/featureSelection/mutinfo/KnnVsKwindowErrorVsMiTheoretical")
#}
```

## Experiment 1

B. C. Ross, “Mutual information between discrete and continuous data sets,” PLoS One, vol. 9, no. 2, p. e87357, Feb. 2014 [Online]. Available: http://dx.doi.org/10.1371/journal.pone.0087357

The first experiment replicates Ross's findings around discretising data & KNN method for estimating MI but using 2 new algorithms and only the smaller sample size

```{r echo=FALSE, cache=TRUE}
devtools::load_all("..")
experiment1 = function(distribution, sampleSize, reps) {
  
  set.seed(101)
  
  kout = NULL
  thMi = distribution$theoreticalMI()
  thMu = distribution$theoreticalMean()
  thSd = sqrt(distribution$theoreticalVariance())
  
  for (i in c(1:reps)) {
	  # i = 1
	  df = distribution$sample(sampleSize)
	  estMean = mean(df$x)
  	estSd = sd(df$x)
  	
  	# SGolay method
	  k = c(5,7,10,15,20,30,40)
	  kresult = sapply(k, function(k2) (calculateDiscreteContinuousMI(df, y, x, method = "SGolay", k_05=k2) %>% pull(I)))
  	kout = kout %>% bind_rows(tibble(
  			method = rep("SGolay",length(k)),
  			param = rep("filter width",length(k)),
  			test = rep(i,length(k)),
  			value = k,
  			estimated.MI = kresult,
  			estimated.Mean = rep(estMean,length(k)),
  			estimated.Sd = rep(estSd,length(k))
  	))
	
		# KWindow method
  	k = c(2:10)
		wresult = sapply(k, function(k2) (calculateDiscreteContinuousMI(df, y, x, method = "KWindow", k_05=k2) %>% pull(I)))
	  kout = kout %>% bind_rows(tibble(
  			method = rep("KWindow",length(k)),
  			param = rep("window width",length(k)),
  			test = rep(i,length(k)),
  			value = k,
  			estimated.MI = wresult,
  			estimated.Mean = rep(estMean,length(k)),
  			estimated.Sd = rep(estSd,length(k))
  	))
	  
	  # KNN method
  	k = c(2:10)
  	if (nrow(df)<500) {
		  w2result = sapply(k, function(k2) (calculateDiscreteContinuousMI(df, y, x, method = "KNN", k_05=k2) %>% pull(I)))
  	} else {
		  w2result = rep(NA,length(k))
		}
	  kout = kout %>% bind_rows(tibble(
  			method = rep("KNN",length(k)),
  			param = rep("knn distance",length(k)),
  			test = rep(i,length(k)),
  			value = k,
  			estimated.MI = wresult,
  			estimated.Mean = rep(estMean,length(k)),
  			estimated.Sd = rep(estSd,length(k))
  	))
	  
	  
	  # Discretise method
	  # binPow = seq(0.1,0.5,0.05)
	  # k = floor(sampleSize^binPow)
	  k = c(4,8,16,32,64,128,256)
	  nresult = sapply(k, function(k2) (calculateDiscreteContinuousMI(df, y, x, method = "DiscretiseByRank", discreteMethod="Histogram", bins=k2) %>% pull(I)))
	  kout = kout %>% bind_rows(tibble(
  			method = rep("Histogram by rank",length(k)),
  			param = rep("number bins",length(k)),
  			test = rep(i,length(k)),
  			value = k, #sampleSize/k,
  			estimated.MI = nresult,
  			estimated.Mean = rep(estMean,length(k)),
  			estimated.Sd = rep(estSd,length(k))
  	))
	  
	  # DiscretiseValue method
	  k = c(4,8,16,32,64,128,256)
	  nresult = sapply(k, function(k2) (calculateDiscreteContinuousMI(df, y, x, method = "DiscretiseByValue", discreteMethod="Histogram", bins=k2) %>% pull(I)))
	  kout = kout %>% bind_rows(tibble(
  			method = rep("Histogram by value",length(k)),
  			param = rep("number bins",length(k)),
  			test = rep(i,length(k)),
  			value = k,
  			estimated.MI = nresult,
  			estimated.Mean = rep(estMean,length(k)),
  			estimated.Sd = rep(estSd,length(k))
  	))
	  
	  # DiscretiseValue method
	  k = c(4,8,16,32,64,128,256)
	  nresult = sapply(k, function(k2) (calculateDiscreteContinuousMI(df, y, x, method = "DiscretiseByValue", discreteMethod="MontgomerySmith", bins=k2) %>% pull(I)))
	  kout = kout %>% bind_rows(tibble(
  			method = rep("Histogram by value",length(k)),
  			param = rep("number bins",length(k)),
  			test = rep(i,length(k)),
  			value = k,
  			estimated.MI = nresult,
  			estimated.Mean = rep(estMean,length(k)),
  			estimated.Sd = rep(estSd,length(k))
  	))
	  
	  # Compression method
	  k = c(1,2)
	  nresult = calculateDiscreteContinuousMI(df, y, x, method = "Compress") %>% pull(I)
	  kout = kout %>% bind_rows(tibble(
  			method = rep("Compress",length(k)),
  			param = rep("no param",length(k)),
  			test = rep(i,length(k)),
  			value = sampleSize/k,
  			estimated.MI = rep(nresult,length(k)),
  			estimated.Mean = rep(estMean,length(k)),
  			estimated.Sd = rep(estSd,length(k))
  	))
	  
  }
  
  kout = kout %>% mutate(
    theoretical.MI = thMi,
  	theoretical.Mean = thMu,
  	theoretical.Sd = thSd
  )
  
  
  
  return(kout)
}

exp1aData = experiment1(distribution=gaussians, sampleSize=400, reps=100)
exp1bData = experiment1(distribution=gaussians, sampleSize=10000, reps=100)
exp1cData = experiment1(distribution=squareWaves, sampleSize=400, reps=100)
exp1dData = experiment1(distribution=squareWaves, sampleSize=10000, reps=100)
exp1eData = experiment1(distribution=lognorm, sampleSize=400, reps=100)
exp1fData = experiment1(distribution=lognorm, sampleSize=10000, reps=100)

```

```{r}
# todo summarise experimental data
plotExperiment1 = function(df) {
  exp1aDataGrouped = df %>% group_by(method,param,value) %>% summarise( #,estimatedMean,estimatedSd,theoreticalMI,theoreticalMean,theoreticalSd) %>% summarise(
    estMIMean = mean(estimated.MI),
    estMIsd = sd(estimated.MI)
  )

  # p3 = 
  return(
    ggplot(exp1aDataGrouped, aes(x=value))+
  		geom_ribbon(aes(ymin=estMIMean-1.96*estMIsd,ymax=estMIMean+1.96*estMIsd), fill = "grey75")+
  		geom_line(aes(y=estMIMean))+ylab("estimated MI")+xlab("free parameter value")+
  		coord_cartesian(ylim = c(0,1))+
      #expand_limits(y=0)+
    	geom_hline(yintercept=min(df$theoretical.MI), colour="blue")+
      facet_wrap(vars(method),scales = "free_x")
  )
}
```


### Experiment 1: gaussians

Using a sample size of 400 and 100 repetitions of the test for bootstrapping

```{r}
plotExperiment1(exp1aData)
standardPrintOutput::saveHalfPageFigure(filename="~/Dropbox/featureSelection/mutinfo/gaussiansEstimated400")
```

Using a sample size of 10000 and 100 repetitions of the test for bootstrapping

```{r}
plotExperiment1(exp1bData)
standardPrintOutput::saveHalfPageFigure(filename="~/Dropbox/featureSelection/mutinfo/gaussiansEstimated10K")
```

### Experiment 1: uniform

Using a sample size of 400 and 100 repetitions of the test for bootstrapping

```{r cache=TRUE}
plotExperiment1(exp1cData)
standardPrintOutput::saveHalfPageFigure(filename="~/Dropbox/featureSelection/mutinfo/squareEstimated400")
```

Using a sample size of 10000 and 100 repetitions of the test for bootstrapping

```{r}
plotExperiment1(exp1dData)
standardPrintOutput::saveHalfPageFigure(filename="~/Dropbox/featureSelection/mutinfo/squareEstimated10K")
```

### Experiment 1: log normal

Using a sample size of 400 and 100 repetitions of the test for bootstrapping

```{r}
plotExperiment1(exp1eData)
standardPrintOutput::saveHalfPageFigure(filename="~/Dropbox/featureSelection/mutinfo/lognormEstimated400")
```

Using a sample size of 10000 and 100 repetitions of the test for bootstrapping

```{r}
plotExperiment1(exp1fData)
standardPrintOutput::saveHalfPageFigure(filename="~/Dropbox/featureSelection/mutinfo/lognormEstimated10K")
```

## Experiment 2

Objectives:
* Assess bias in estimates versus MI
** Random distributions
** Plot estimated versus theoretical MI
** Plot estimated versus theoretical Mean & Variance
** Corrected versus uncorrected?
*** Not done: solve estimated versus theoretical for parameters a&b for random distributions & samples


```{r echo=FALSE}
devtools::load_all("..")
summariseError2 = function(df) {
  return(
    df %>% filter(!is.na(relativeError)) %>% group_by(sample,distributions,param) %>% summarise(
      theoretical = mean(theoretical, na.rm=TRUE),
      rmse = sqrt(mean(absoluteError^2, na.rm=TRUE)),
      mae = mean(absoluteError, na.rm=TRUE),
      vae = var(absoluteError, na.rm=TRUE),
      nmae = mean(relativeError, na.rm=TRUE),
      nvae = var(relativeError, na.rm=TRUE),
      median_ae = quantile(absoluteError, probs=c(0.5), names=FALSE, na.rm=TRUE),
      upper_iqr_ae = quantile(absoluteError, probs=c(0.75), names=FALSE, na.rm=TRUE),
      lower_iqr_ae = quantile(absoluteError, probs=c(0.25), names=FALSE, na.rm=TRUE)
    )
  )
}

experiment2 = function(reps,meth = c("KWindow","KNN","SGolay","Discrete","DiscreteValue","Compress")) {
  
  set.seed(101)
  result = NULL
  
  for (i in c(1:reps)) {
    
    ndist = sample(2:4,1)
    distribution = ConditionalDistribution$new()$withRandomDistributions(ndist)
    thMi = tryCatch(distribution$theoreticalMI(),error=function(e) return(NA))
    #thMu = distribution$theoreticalMean()
    #thSd = sqrt(distribution$theoreticalVariance())
    
    df = distribution$sample((sample.int(6,1)+1)*100)
    
    estMi = sapply(meth, function(m) calculateDiscreteContinuousMI(df, y, x, method = m) %>% pull(I))
    #estMean = mean(df$x)
    #estSd = sd(df$x)
    
    result = result %>% bind_rows(
      tibble(
        sample = rep(i,length(meth)),#+2),
        distributions = rep(ndist,length(meth)),#+2),
        sampleSize = rep(nrow(df),length(meth)),
        param = meth,
        theoretical = rep(thMi,length(meth)),
        estimated = estMi,
        #theoretical = c(rep(thMi,length(meth)),thMu,thSd),
        #estimated = c(estMi,estMean,estSd),
        absoluteError = estimated-theoretical,
        relativeError = ifelse(theoretical<0.05,NA,absoluteError/theoretical)
      )
    )
    
  }
  
  return(result)
}



exp2adata = experiment2(1000)

```

```{r}
# todo summarise experimental data
# TODO: plot relative error vs. theoretical MI
plotExperiment2 = function(df) {
  # p3 = 
  return(
    ggplot(df %>% mutate(components=as.factor(distributions)), aes(x=theoretical,y=estimated,colour=as.factor(sampleSize)))+
  		geom_point(size=2,stroke=0,alpha=0.2)+
  		geom_abline(slope=1,intercept=0,colour="grey75")+
  		coord_cartesian(ylim = c(0,1),xlim = c(0,1))+
    	facet_wrap(vars(param),scales = "free")+labs(colour="Sample size")
  )
}
```

### Experiment 2 part one - Error plots

```{r cache=TRUE}
plotExperiment2(exp2adata)
standardPrintOutput::saveHalfPageFigure(filename="~/Dropbox/featureSelection/mutinfo/errorEstimatesVsTheoretical")
```

### Experiment 2 - part 2 

Perform a paired t-test on prediction and theoretical value

```{r}
tmp2 = exp2adata %>% group_by(param) %>% group_modify(function(d,...) {
    tResult = t.test(d$theoretical, d$estimated, paired=TRUE)
    tibble(
      effectSize = twoDp(tResult$estimate),
      confidenceInt = paste0(twoDp(tResult$conf.int),collapse=", "),
      pValue = twoDp(tResult$p.value)
    )
}) %>% rename(`Estimate`=param, Difference = effectSize, `Confidence interval`=confidenceInt, `P value` = pValue ) %>% ungroup() %>% standardPrintOutput::mergeCells() 
tmp2
tmp2 %>% standardPrintOutput::saveTable(filename = "~/Dropbox/featureSelection/mutinfo/adjustedError2")
# ggplot(df, aes)
```

# Experiment 3

Objectives:
* Look at sample size versus accuracy
** MI, Mean and SD with increasing sample size
** regression to predict SD of MI from est mean versus theoretical mean  & est versus theoretical SD

```{r echo=FALSE, cache=TRUE}
devtools::load_all("..")
experiment3 = function(distribution, reps, meth = c("KWindow","KNN","SGolay","Discrete","DiscreteValue","Compress")) {
  
  set.seed(101)
  result = NULL
  
  thMi = distribution$theoreticalMI()
  thMu = distribution$theoreticalMean()
  thSd = sqrt(distribution$theoreticalVariance())
  
  j=0
  for (samples in c(8,16,32,64,128,256,512,1024,2048,4096,8192,16384)) {
  
    for (i in c(1:reps)) {
      
      j=j+1
      
      df = distribution$sample(samples)
      minGroupSize = min(df %>% group_by(y) %>% count() %>% pull(n))
      estMi = sapply(meth, function(m) calculateDiscreteContinuousMI(df, y, x, method = m) %>% pull(I))
      estMean = mean(df$x)
      estSd = sd(df$x)
    
      result = result %>% bind_rows(
        tibble(
          id = j,
          minGroupSize = minGroupSize,
          sample = samples,
          param = c(meth,"Mean","Std deviation"),
          theoretical = c(rep(thMi,length(estMi)),thMu,thSd),
          estimated = c(estMi,estMean,estSd)
        )
      )
    }
  }
  
  return(result)
}

# setup error

quantifyError = function(df) {
  return(
    df %>% mutate(
      absoluteError = estimated-theoretical,
      relativeError = ifelse(theoretical<0.05,NA,absoluteError/theoretical)
    ) %>% filter(!is.na(relativeError)) %>% group_by(sample,param) %>% summarise(
      minGroupSize =mean(minGroupSize),
      theoretical = mean(theoretical, na.rm=TRUE),
      rmse = sqrt(mean(absoluteError^2, na.rm=TRUE)),
      mae = mean(absoluteError, na.rm=TRUE),
      vae = var(absoluteError, na.rm=TRUE),
      nmae = mean(relativeError, na.rm=TRUE),
      nvae = var(relativeError, na.rm=TRUE),
      median_ae = quantile(absoluteError, probs=c(0.5), names=FALSE, na.rm=TRUE),
      upper_iqr_ae = quantile(absoluteError, probs=c(0.75), names=FALSE, na.rm=TRUE),
      lower_iqr_ae = quantile(absoluteError, probs=c(0.25), names=FALSE, na.rm=TRUE)
    )
  )
}



exp3aData = experiment3(lognorm,100)
exp3bData = experiment3(gaussians,100)
exp3cData = experiment3(squareWaves,100)
```

```{r}
# todo summarise experimental data
plotExperiment3 = function(df, components) {
  summary = df %>% group_by(sample,param) %>% quantifyError()
  return(ggplot(summary %>% filter(param %in% components),aes(x=sample))+
  		geom_line(aes(y=mae),colour="blue")+
  		geom_ribbon(aes(ymin=mae-1.96*vae,ymax=mae+1.96*vae),fill="blue",alpha=0.1)+
  		#geom_line(aes(y=estimatedMedian),colour="grey75")+
  		#geom_ribbon(aes(ymin=estimatedLower,ymax=estimatedUpper),fill="grey75",alpha=0.1)+
  		coord_cartesian(ylim = c(-0.25,0.25))+
      geom_hline(yintercept=0, colour="red")+
  		ylab("absolute error")+
  		xlab("sample size")+
    	facet_wrap(vars(param))+scale_x_log10())
}
```

```{r}
# devtools::load_all("..")

plotExperiment3(exp3aData,c("Mean","Std deviation"))
plotExperiment3(exp3aData,c("KWindow","KNN","SGolay","Discrete","DiscreteValue","Compress"))
standardPrintOutput::saveThirdPageFigure(filename="~/Dropbox/featureSelection/mutinfo/bootstrappingLogNorm")
```

```{r}
#devtools::load_all("..")

plotExperiment3(exp3bData,c("Mean","Std deviation"))
plotExperiment3(exp3bData,c("KWindow","KNN","SGolay","Discrete","DiscreteValue","Compress"))
standardPrintOutput::saveThirdPageFigure(filename="~/Dropbox/featureSelection/mutinfo/bootstrappingGaussians")
```

```{r}
# devtools::load_all("..")

plotExperiment3(exp3cData,c("Mean","Std deviation"))
plotExperiment3(exp3cData,c("KWindow","KNN","SGolay","Discrete","DiscreteValue","Compress"))
standardPrintOutput::saveThirdPageFigure(filename="~/Dropbox/featureSelection/mutinfo/bootstrappingUniform")
```