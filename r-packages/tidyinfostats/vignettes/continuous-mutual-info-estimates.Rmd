---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(infotheo)
library(tidyverse)
library(ggplot2)
library(devtools)
#library(tidyinfostats)
devtools::load_all("..")
set.seed(101)
theme_set(standardPrintOutput::defaultFigureLayout())
```

## Mutual information of continuous versus discrete disctirbutions

Assume a continuous distribution

```{r}
hb = ConditionalDistribution$new()
hb$withDistribution(0.75, LogNormalDistribution$new(mode=12,sd=2), "asymptomatic")
hb$withDistribution(0.25, LogNormalDistribution$new(mode=8,sd=3), "tired")
hb$withDistribution(0.25, LogNormalDistribution$new(mode=4,sd=5), "unwell")
hb$plot(0,20)

```

And another

```{r}
k = ConditionalDistribution$new()
k$withDistribution(0.125, NormalDistribution$new(mean=1,sd=0.5), "unwell")
k$withDistribution(0.75, NormalDistribution$new(mean=2,sd=1), "asymptomatic")
k$withDistribution(0.125, NormalDistribution$new(mean=8,sd=3), "tired")
```

We can use the properties of the underlying distributions to calculate theoretical values for the Mean, Variance and Mutual Information

```{r}
tibble(measure = c("Mean","Variance","Mutual Information"),
hb = c(hb$theoreticalMean(),hb$theoreticalVariance(), hb$theoreticalMI()),
k = c(k$theoreticalMean(),k$theoreticalVariance(),k$theoreticalMI()))
```

We can also generate test data by samplinf from the underlying distributions

```{r}
set.seed(101)
testData = k$sample(500) %>% mutate(test="k") %>% bind_rows(
  hb$sample(1000) %>% mutate(test="hb")) %>% rename(outcome=y, value=x)

testData = testData[sample(nrow(testData)),]

ggplot(testData, aes(fill=outcome, x=value))+geom_histogram(position="dodge", bins=50)+facet_grid(cols=vars(test))
```

And use this sample to estimate the MI using 3 methods.

TODO: Explain the methods.

```{r}
devtools::load_all("..")
#debug(calculateDiscreteDiscreteMI)
#undebug(calculateDiscreteContinuousMI_KNN)
#debug(calculateDiscreteDiscreteMI_Entropy)
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "KWindow")
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "SGolay")
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByRank")
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByValue", discreteMethod="Entropy", entropyMethod="InfoTheo")
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByValue", discreteMethod="Histogram")
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByValue", discreteMethod="MontgomerySmith", j=2)
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "KNN")
set.seed(101)
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "Compression")
set.seed(101)
testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByRank", binStrategy = linearBySize(8,4,256), discreteMethod="Entropy", entropyMethod="Compression")
tibble(test=c("hb","k"),I = c(hb$theoreticalMI(),k$theoreticalMI()), I_sd=NA,method=rep("theoretical",2))


# left_join(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "Entropy") %>% rename(I.emp=I), by="test") %>%
```

Check entropy calculations working:


```{r}
devtools::load_all("..")

dists = list(
  normal = NormalDistribution$new(mean = 1, sd = 0.5),
  logNormal = LogNormalDistribution$new(mode = 1, sd = 0.5),
  uniform = UniformDistribution$new(min = 0,max = 2)
)

dists$normal$plot(-2,5)
```

```{r}
devtools::load_all("..")
#debug(discretise_ByRank)
#debug(calculateEntropy_Histogram)
# options(error = browser)

dists = list(
  normal = NormalDistribution$new(mean = 1, sd = 0.5),
  logNormal = LogNormalDistribution$new(mode = 1, sd = 0.5),
  uniform = UniformDistribution$new(min = 0,max = 2)
)

bins = 24
cutsDf = tibble(cut=seq(-1,3,length.out = bins-1))

out = NULL
method = c("MontgomerySmith", "Histogram","InfoTheo","Compression")
for (dist in dists) {
  thEnt = dist$theoreticalEntropy()
  for (i in c(1:10)) { #0)) {
    for (j in c(1:10,20,40,60,80,100)) { #,15,20,25,30,35,40,45,50,60,70,80,90,100)) {
      s = dist$sample(n=10*j)
      s = s %>% discretise(x, x_discrete, method="Manual", cutsDf=cutsDf)
      #s = s %>% discretise(x, x_discrete, method="ByRank")
      for (m in method) {
        estimate = calculateEntropy(s, groupVars=vars(x_discrete), method = m, infoTheoMethod = "mm") %>% pull(H)
        out = out %>% bind_rows(tibble(
          N = 10*j,
          method = m,
          dist = dist$label(),
          estimate = estimate,
          theoretical = thEnt
        ))
      }
    }
  }
}

# TODO: the theoretical mutual information is calculated using an integral which assumes equal bin width
# There is probably an adjustment that involves log(Number of non empty bins) and log(Number of bins) that 
# would adjust the estimated values to come into line with the theoretical.
# not sure this really works

out.summ = out %>% group_by(N,method,dist) %>% summarise(
  mean = mean(estimate),
  sd = sd(estimate),
  #th = max(log(7/theoretical))
  #th = max(log(20)-theoretical)
  #th = max(log(bins*theoretical))
  th = max(theoretical*log(bins))
  #th = max(log(bins)*theoretical)
)

ggplot(out.summ, aes(x=N,colour=method,fill=method))+
  geom_line(aes(y=mean))+
  geom_ribbon((aes(ymin=mean-1.96*sd, ymax=mean+1.96*sd)),colour=NA,alpha=0.2)+
  geom_hline(aes(yintercept = th))+
  geom_hline(yintercept=log(bins),colour="grey50")+
  facet_wrap(vars(dist))
```

Theoretical entropy values
Head and tails:

```{r}
devtools::load_all("..")

compress = NULL
for (Cpow in c(2:8)) {
  for (binPow in c(2:12)) {
    C = 2^Cpow
    size = 2^binPow
    for (rep in c(1:10)) {
      C1 = length(memCompress(as.raw(sample.int(C,size,replace = TRUE)-1)))
      compress = compress %>% rbind(tibble(
        i = rep,
        bits = Cpow,
        size = size,
        C1 = C1
      ))
    }
  }
}

compress.summ = compress %>% group_by(bits,size) %>% summarise(
  max_C1 = max(C1),
  min_C1 = min(C1),
  mean_C1 = mean(C1)
)

ggplot(compress.summ,aes(x=log2(size),y=log2(max_C1),colour=as.factor(bits)))+
  geom_line()
  #geom_point()+geom_smooth(method="lm") # log2(y),colour=max(C1)))+geom_tile()

compress.model = compress.summ %>% ungroup() %>% mutate(logSize = log2(size), bits=bits, logMaxC1 = log2(max_C1))

summary(lm(logMaxC1 ~ bits+logSize, data = compress.model))

maxC1 = function(size,classes) {2^(0.78175+0.14477*log2(classes)+0.78537*log2(size))}
maxC1(1024,256)
```
