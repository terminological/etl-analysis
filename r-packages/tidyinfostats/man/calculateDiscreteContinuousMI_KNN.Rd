% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tidyDiscreteContinuousMI.R
\name{calculateDiscreteContinuousMI_KNN}
\alias{calculateDiscreteContinuousMI_KNN}
\title{calculate mutual information between a categorical value (X) and a continuous value (Y) using a sliding window and local entropy measure}
\usage{
calculateDiscreteContinuousMI_KNN(
  df,
  discreteVar,
  continuousVar,
  k_05 = 4,
  useKWindow = TRUE,
  ...
)
}
\arguments{
\item{df}{- may be grouped, in which case the value is interpreted as different types of continuous variable}

\item{discreteVar}{- the column of the categorical value (X)}

\item{continuousVar}{- the column of the continuous value (Y)}

\item{k_05}{- half the sliding window width - this should be a small number like 1,2,3.}

\item{useKWindow}{- will switch to using the much faster KWindow estimator for larger sample sizes (>500) when the difference between the 2 methods is negligable}
}
\value{
a dataframe containing the disctinct values of the groups of df, and for each group a mutual information column (I). If df was not grouped this will be a single entry
}
\description{
This is an implementation of the technique described here:
}
\details{
B. C. Ross, “Mutual information between discrete and continuous data sets,” PLoS One, vol. 9, no. 2, p. e87357, Feb. 2014 [Online]. Available: http://dx_doi.org/10.1371/journal.pone.0087357

But it is very slow. Empirically it also does not give any better estimate that the KWindow method.
}
